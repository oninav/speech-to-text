[
  {
    "objectID": "NLP Speech.html#speech-to-text-conversion",
    "href": "NLP Speech.html#speech-to-text-conversion",
    "title": "",
    "section": "Speech-to-Text Conversion",
    "text": "Speech-to-Text Conversion\nTechniques, Tools, and Trade-offs\nOmkar Ninav\nJune 30, 2025\n\nGood [morning/afternoon], thank you for taking the time to attend this presentation.\nThe goal today is to give a structured and technical overview of speech-to-text conversion â€” how it works, what tools and models are currently leading the field, and what trade-offs are involved in choosing a particular solution.\nThis is a technology thatâ€™s become foundational in a wide range of domains â€” from customer support and assistive tech, to real-time captioning and voice interfaces. Iâ€™ll be focusing on how Python-based tools like Whisper, Wav2Vec2, and others can be used to build robust STT systems.\nBy the end, I hope to outline clearly: - How the core pipeline functions, - Which models are best suited for various use cases, and - What the cost and accuracy implications are â€” especially in terms of practical deployment.\nLetâ€™s begin with the basics."
  },
  {
    "objectID": "NLP Speech.html#what-is-speech-to-text",
    "href": "NLP Speech.html#what-is-speech-to-text",
    "title": "",
    "section": "What is Speech-to-Text?",
    "text": "What is Speech-to-Text?\n\nConverts spoken language into written text\nAlso known as Automatic Speech Recognition (ASR)\nUsed in applications like:\n\nVirtual assistants (e.g., Siri, Alexa)\nCaptioning & subtitling\nMeeting transcription\nVoice commands\n\n\n\nSpeech-to-text, also called Automatic Speech Recognition or ASR, is the process of converting human speech into readable, written text.\nAt a high level, itâ€™s the core technology behind systems like Siri, Alexa, Google Assistant, and any app that takes voice input.\nBeyond virtual assistants, itâ€™s widely used in live captioning, automated meeting transcription, voice commands in smart devices, and even accessibility tools for people with disabilities.\nIn recent years, thanks to deep learning and advances in compute power, STT systems have become far more accurate and accessible â€” even for developers like us working with open-source tools.\nIn the next slide, Iâ€™ll walk through the high-level architecture of how speech-to-text systems are typically designed."
  },
  {
    "objectID": "NLP Speech.html#how-speech-to-text-works",
    "href": "NLP Speech.html#how-speech-to-text-works",
    "title": "",
    "section": "How Speech-to-Text Works",
    "text": "How Speech-to-Text Works\n\nAudio Input\nVoice signal captured from a microphone or file\nFeature Extraction\nConverts raw audio into numerical features (e.g., MFCCs, spectrograms)\nAcoustic Model\nMaps features to phonemes or characters using ML/DL models\nLanguage Model\nPredicts word sequences for context-aware transcription\nDecoder\nAligns acoustic and language models to produce final text\n\n\n\n\n\n\nflowchart LR\n  A[Audio Input&lt;br/&gt;Voice signal] --&gt; B[Feature Extraction&lt;br/&gt;MFCCs / Spectrograms]\n  B --&gt; C[Acoustic Model&lt;br/&gt;ML/DL-based]\n  C --&gt; D[Language Model&lt;br/&gt;Contextual Prediction]\n  D --&gt; E[Decoder&lt;br/&gt;Final Text Output]\n\n\n\n\n\n\n\nThis slide breaks down the standard architecture behind most modern speech-to-text systems.\nIt starts with the audio input, typically a microphone recording or audio file, which is passed to a feature extraction module. This module transforms the raw waveform into meaningful patterns â€” usually through methods like MFCCs (Mel-Frequency Cepstral Coefficients) or spectrograms.\nNext, the acoustic model interprets these features and maps them to likely phonemes or character sequences. Traditionally, these were HMM-based, but today theyâ€™re often deep neural networks like CNNs, RNNs, or transformers.\nThe language model plays a critical role â€” it uses linguistic context to predict the most likely word sequences. This is what helps the system understand the difference between similar-sounding phrases like â€œice creamâ€ vs â€œI scream.â€\nFinally, the decoder combines all of this â€” the acoustic predictions and the language probabilities â€” to produce the final transcription.\nUnderstanding this pipeline helps clarify where different models and libraries fit in, and what we can tweak based on accuracy, speed, or deployment constraints."
  },
  {
    "objectID": "NLP Speech.html#techniques-traditional-asr",
    "href": "NLP Speech.html#techniques-traditional-asr",
    "title": "",
    "section": "Techniques: Traditional ASR",
    "text": "Techniques: Traditional ASR\n\nHMMs + GMMs\nEarly models for mapping acoustic features to phonemes\nn-gram Language Models\nPredict word sequences based on prior word probabilities\nFeature Extraction (MFCCs)\nConverts raw audio into spectral features\n\n\nLetâ€™s start with traditional ASR systems.\nEarly speech recognition was built using Hidden Markov Models (HMMs) and Gaussian Mixture Models (GMMs). HMMs model the sequential nature of speech, and GMMs handle the probability distributions of acoustic features within each state.\nFor predicting the next word or phrase, systems used n-gram language models â€” for example, a trigram model would look at the two previous words to estimate the next one. These models were easy to implement but had limited ability to understand context or long-term dependencies.\nThe front end of these systems used MFCCs (Mel-Frequency Cepstral Coefficients) to convert audio waveforms into feature vectors that captured the shape and tone of the speech signal. MFCCs remain popular even in some deep learning systems due to their compact and informative representation of sound.\nWhile these traditional methods laid the foundation for modern ASR, they struggled with complex variations like accents, overlapping speakers, and noisy environments."
  },
  {
    "objectID": "NLP Speech.html#techniques-deep-learning-modern-trends",
    "href": "NLP Speech.html#techniques-deep-learning-modern-trends",
    "title": "",
    "section": "Techniques: Deep Learning & Modern Trends",
    "text": "Techniques: Deep Learning & Modern Trends\n\nRNNs, CNNs, Transformers\nDeep models for sequence modeling and attention\nEnd-to-End Models (CTC / Attention)\nLearn transcription directly from audio input\nTrends\n\nSelf-supervised learning (e.g.Â Wav2Vec 2.0)\nMultilingual support (e.g.Â Whisper)\nOn-device inference (edge devices, mobile)\n\n\n\nModern speech recognition systems have shifted toward deep learning.\nInitially, Recurrent Neural Networks (RNNs) were used because they could model sequences â€” perfect for speech. Then came Convolutional Neural Networks (CNNs) to handle time-frequency patterns in audio. Today, transformers are dominant due to their superior performance in modeling long-range dependencies and context.\nInstead of breaking the process into multiple modules like traditional ASR, deep learning systems often use end-to-end models. These can be trained to map directly from raw audio to text using techniques like Connectionist Temporal Classification (CTC) or attention-based models.\nRecent trends in STT are pushing the field even further: - Self-supervised learning enables models like Wav2Vec 2.0 to learn from raw, unlabeled audio â€” drastically reducing the need for costly transcriptions. - Models like Whisper offer multilingual support and are robust to accents and background noise. - Thereâ€™s also growing interest in on-device inference, where models are small and efficient enough to run on phones, edge devices, or embedded systems â€” ensuring both privacy and speed.\nThese modern techniques make STT more accurate, scalable, and accessible than ever before."
  },
  {
    "objectID": "NLP Speech.html#python-libraries-for-speech-to-text-12",
    "href": "NLP Speech.html#python-libraries-for-speech-to-text-12",
    "title": "",
    "section": "Python Libraries for Speech-to-Text (1/2)",
    "text": "Python Libraries for Speech-to-Text (1/2)\n\nWhisper (OpenAI)\n\nTransformer-based, multilingual\nHigh accuracy, runs offline\n\nSpeechRecognition\n\nSimple API wrapper for Google, IBM, etc.\nEasy for beginners\n\nWav2Vec 2.0 (Hugging Face)\n\nPretrained self-supervised models\nHigh-quality transcriptions\n\n\n\nThis slide introduces three powerful Python tools used in speech-to-text development.\nWhisper is one of the best-performing open-source models available. It supports multilingual audio, works offline, and is robust to noise and accents. It does require a decent GPU, but the results are exceptional.\nSpeechRecognition is simple and wraps around cloud APIs like Google or IBM Watson. Itâ€™s great for quick demos or for beginners but is not suitable for offline or secure environments.\nWav2Vec 2.0 is built by Meta and available on Hugging Face. Itâ€™s transformer-based, pre-trained on massive audio corpora, and can deliver near state-of-the-art accuracy, especially on clean English audio."
  },
  {
    "objectID": "NLP Speech.html#python-libraries-for-speech-to-text-22",
    "href": "NLP Speech.html#python-libraries-for-speech-to-text-22",
    "title": "",
    "section": "Python Libraries for Speech-to-Text (2/2)",
    "text": "Python Libraries for Speech-to-Text (2/2)\n\nDeepSpeech (Mozilla)\n\nLightweight and fast\nLess accurate on noisy inputs\n\nKaldi (via PyKaldi)\n\nResearch-grade toolkit\nSteeper learning curve\n\nVosk\n\nReal-time offline STT\nWorks on Raspberry Pi, Android, desktops\nMultilingual, very lightweight\n\n\n\nDeepSpeech, from Mozilla, was designed for simplicity and edge deployment. Itâ€™s fast and easy to install, but less accurate in noisy or real-world conditions compared to newer models.\nKaldi is a highly flexible and powerful toolkit often used in academia or enterprise. It offers full control over the speech recognition pipeline, including acoustic and language models. However, itâ€™s not beginner-friendly and requires significant setup effort. The Python wrapper PyKaldi helps but still demands technical depth.\nVosk is an often-overlooked gem. It supports real-time offline transcription in 20+ languages, including Hindi, Marathi, and more. It runs even on low-resource devices like Raspberry Pi or Android phones. Itâ€™s ideal for small, local deployments where Whisper would be too heavy or cloud is not an option.\nSplitting the content across two slides avoids overcrowding and gives us room to discuss each tool more clearly."
  },
  {
    "objectID": "NLP Speech.html#model-comparison-features-at-a-glance",
    "href": "NLP Speech.html#model-comparison-features-at-a-glance",
    "title": "",
    "section": "Model Comparison: Features at a Glance",
    "text": "Model Comparison: Features at a Glance\n\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nOffline\nMultilingual\nEase of Use\nCost\n\n\n\n\nWhisper\nâœ…âœ…âœ…\nâœ…\nâœ…âœ…âœ…\nâœ…âœ…\nFree\n\n\nWav2Vec 2.0\nâœ…âœ…\nâœ…\nâš ï¸ (mostly English)\nâœ…\nFree\n\n\nGoogle API\nâœ…âœ…âœ…\nâŒ\nâœ…âœ…âœ…\nâœ…âœ…âœ…\nPaid\n\n\nDeepSpeech\nâœ…\nâœ…\nâŒ\nâœ…âœ…\nFree\n\n\nKaldi\nâœ…âœ…\nâœ…\nâœ… (with effort)\nâš ï¸ Complex\nFree\n\n\nVosk\nâœ…âœ…\nâœ…\nâœ…âœ…\nâœ…âœ…âœ…\nFree\n\n\n\n\nThis table summarizes the strengths and limitations of some of the most widely used speech-to-text models and toolkits.\nStarting with Whisper, it stands out as a balanced option â€” it offers high accuracy, works offline, supports multiple languages, and is open-source. It does require a decent GPU to run efficiently, especially for long audio files or real-time use, but itâ€™s very accessible for developers and teams with modest resources.\nWav2Vec 2.0 is another excellent option. It provides high accuracy, especially when fine-tuned on domain-specific data. However, itâ€™s primarily trained on English datasets, so multilingual support is more limited unless you retrain it. Itâ€™s open-source and can be accessed via Hugging Face.\nGoogle Speech-to-Text API is a commercial solution. It offers very high accuracy and supports over 100 languages. Itâ€™s excellent for real-time use cases and scales easily. However, itâ€™s not free â€” the cost is around $1.44 per hour, and it requires internet connectivity, which may raise privacy concerns in sensitive environments.\nDeepSpeech was designed to be lightweight and fast, making it suitable for edge devices. That said, it has started to lag behind in terms of accuracy, especially on noisy or accented inputs, and its development has slowed down in recent years.\nKaldi is extremely powerful and flexible. Itâ€™s used in many academic and enterprise applications where low-level control is needed â€” for instance, custom language models, speaker adaptation, and complex pipelines. However, itâ€™s not beginner-friendly, and the learning curve is steep. PyKaldi can help if you want to work with it in Python.\nVosk is surprisingly effective given its small size. It offers decent accuracy, real-time performance, works fully offline, and is extremely easy to integrate â€” especially on embedded systems or mobile platforms.For projects targeting Indian languages or edge devices, Vosk can be a better fit than Whisper or cloud APIs.\nThis slide helps us decide which model might be most appropriate depending on our project constraints â€” whether we prioritize cost, offline access, multilingual support, or ease of use.\nIn the next slide, weâ€™ll break down the cost aspect more specifically, including free tiers and usage pricing for cloud APIs."
  },
  {
    "objectID": "NLP Speech.html#cost-open-source-models",
    "href": "NLP Speech.html#cost-open-source-models",
    "title": "",
    "section": "Cost: Open-Source Models",
    "text": "Cost: Open-Source Models\n\n\n\nModel\nCost\nOffline Use\nCloud Required\n\n\n\n\nWhisper\nFree (Open Source)\nâœ…\nâŒ\n\n\nWav2Vec 2.0\nFree (Open Source)\nâœ…\nâŒ\n\n\nDeepSpeech\nFree (Open Source)\nâœ…\nâŒ\n\n\nKaldi\nFree (Open Source)\nâœ…\nâŒ\n\n\nVosk\nFree (Open Source)\nâœ…\nâŒ\n\n\n\n\nAll of these models are completely free and can run offline. This gives us full control over the transcription pipeline, which is useful for privacy-sensitive environments or when working with large amounts of audio data.\nWhisper is the most accurate among these and works well out of the box. Wav2Vec 2.0 is also strong, especially with fine-tuning. DeepSpeech is lightweight but older, and Kaldi offers research-grade customization â€” though with a steep learning curve. Vosk, in particular, is very lightweight. It can run on mobile devices and Raspberry Pi, making it ideal for resource-constrained environments.\nThe key point here is: these tools have no recurring costs and no internet dependency â€” ideal for secure or offline environments."
  },
  {
    "objectID": "NLP Speech.html#cost-cloud-apis",
    "href": "NLP Speech.html#cost-cloud-apis",
    "title": "",
    "section": "Cost: Cloud APIs",
    "text": "Cost: Cloud APIs\n\n\n\n\n\n\n\n\n\nService\nApprox. Cost\nOffline Use\nCloud Required\n\n\n\n\nGoogle Speech API\n~$1.44 per hour\nâŒ\nâœ…\n\n\nAWS Transcribe\n~$1.44 per hour\nâŒ\nâœ…\n\n\nAzure Speech\n~$1.60 per hour\nâŒ\nâœ…\n\n\n\n\nCloud APIs offer high performance and easy integration, but they come at a cost.\nThese services typically charge around $1.44 to $1.60 per hour of audio processed. The pricing is usually pay-as-you-go, and may include limited free tiers initially.\nThey are useful for real-time transcription, multi-language support, and when we donâ€™t want to manage infrastructure ourselves.\nHowever, they require internet access, and data is transmitted to external servers, which might raise privacy or compliance concerns.\nSo while cloud APIs are powerful, we need to carefully evaluate whether the convenience justifies the ongoing cost and cloud dependency."
  },
  {
    "objectID": "NLP Speech.html#cost-summary-recommendation",
    "href": "NLP Speech.html#cost-summary-recommendation",
    "title": "",
    "section": "Cost â€“ Summary & Recommendation",
    "text": "Cost â€“ Summary & Recommendation\n\nOpen-Source Models (Whisper, Wav2Vec, etc.)\n\nâœ… Free and offline\nâš ï¸ Require setup and local resources\n\nCloud APIs (Google, AWS, Azure)\n\nâœ… Easy to use, scalable\nâš ï¸ Ongoing cost and privacy trade-offs\n\n\nâœ”ï¸ Recommendation:\nUse Whisper or Wav2Vec 2.0 for local, cost-effective transcription\nUse Vosk for light, multilingual offline STT (e.g.Â edge devices)\nUse Cloud APIs only for real-time or highly multilingual needs\n\nTo summarize, open-source models like Whisper and Wav2Vec are ideal when we want a free, offline, and private solution. They do require more local compute and some initial setup, but the cost savings and control can be significant â€” especially for batch processing or secure applications.\nCloud APIs are best suited when we need real-time transcription, scalability, or support for multiple languages out of the box. However, they incur ongoing costs and may raise data privacy issues.\nFor most internal use cases or offline processing, Whisper offers the best trade-off. For external client-facing products, cloud APIs can be considered with proper cost tracking."
  },
  {
    "objectID": "NLP Speech.html#final-decision",
    "href": "NLP Speech.html#final-decision",
    "title": "",
    "section": "âœ… Final Decision",
    "text": "âœ… Final Decision\nChosen Model: openai/whisper-medium\nğŸ”¹ Why Whisper Medium?\n\nâœ… Good balance between accuracy and inference speed\nâœ… Lower VRAM & compute requirements than large models\nâœ… Suitable for local deployment (runs on RTX 4050)\nâœ… Consistent performance across test cases\nâœ… Robust to mild accents and moderate background noise\n\nğŸ”¸ Not Chosen:\n\nwhisper-large: Higher accuracy but heavier (more VRAM)\nwhisper-tiny / base: Faster, but less accurate\n\n\nAfter testing several models, weâ€™ve decided to go with Whisper Medium.\nIt provides a strong balance between accuracy and speed, especially on a GPU like RTX 4050. While the large model performed slightly better in accuracy, its inference time and GPU memory use were significantly higher â€” which isnâ€™t ideal for our production use.\nThe tiny and base models were faster, but dropped too much in transcription quality, especially for longer or more technical audio.\nWhisper Medium gives us consistent results and can run fully offline, which fits both our deployment and privacy goals. Itâ€™s the best fit given our constraints and performance requirements."
  },
  {
    "objectID": "NLP Speech.html#transcript-evaluation-whisper-vs-original",
    "href": "NLP Speech.html#transcript-evaluation-whisper-vs-original",
    "title": "",
    "section": "ğŸ“ Transcript Evaluation â€“ Whisper vs Original",
    "text": "ğŸ“ Transcript Evaluation â€“ Whisper vs Original\nğŸ™ï¸ Sample: NPTEL Lecture â€“ Literature History\nğŸ“Œ Original:\n&gt; At the outset, this being the first session, it is very important to give an overview of the course. This course is spread over 12 weeks and we may have 30 hours of teaching involved in this. Let me also introduce you to the objectives of this course so that the intentions become clearer.\nğŸŒ€ Whisper Output:\n&gt; At the outset, this being the first session, it is very important to give an overview of the course. This course is spread over 12 weeks and we may have 30 hours of teaching involved in this and we also introduce you to the objectives of this course so that the intentions become more clear.\nğŸ”¹ âœ… Core content retained\nğŸ”¹ âš ï¸ Minor stylistic variation â€“ rephrasing & joined sentences"
  },
  {
    "objectID": "NLP Speech.html#section",
    "href": "NLP Speech.html#section",
    "title": "",
    "section": "",
    "text": "ğŸ§  Observation Summary\n\nWhisper captured all key points with high fidelity\nPunctuation differences due to lack of postprocessing\nFiller words (â€œwe alsoâ€) inserted naturally from speech\nIdeal for lecture summarization, accessibility, or note generation\n\n\nIn this real-world evaluation using an NPTEL lecture, Whisper accurately transcribed complex academic content, maintaining meaning and structure.\nThe differences observed are stylistic, not factual. Punctuation is less formal, and thereâ€™s a natural flow of spoken language like â€œwe also introduceâ€¦â€ instead of â€œLet me also introduceâ€¦â€\nThis shows that Whisper is reliable for lecture or classroom transcription, even in moderately accented or Indian English speech. It preserves intent and phrasing even with unedited audio input."
  },
  {
    "objectID": "NLP Speech.html#whisper-accuracy",
    "href": "NLP Speech.html#whisper-accuracy",
    "title": "",
    "section": "ğŸ“Š Whisper Accuracy",
    "text": "ğŸ“Š Whisper Accuracy\nğŸ”¹ Quantitative Evaluation\n\nSemantic Similarity (cosine): 0.9766 âœ…\nROUGE-1 F1 Score: 0.9714\n\nROUGE-L F1 Score: 0.9524\n\nWord Error Rate (WER): 12.43%\n\nğŸ” Interpretation\n\nâœ… Very high semantic match â€“ meaning fully preserved\n\nâœ… Low WER â€“ minor word-level issues\n\nâœ… ROUGE scores confirm strong overlap in phrase structure\n\nâš ï¸ Differences mostly in punctuation, filler words, and phrasing\n\n\nIn this test, Whisper Medium showed outstanding performance on an NPTEL academic lecture.\nA semantic similarity score of 0.9766 indicates that the transcript preserves nearly all the original meaning. ROUGE-1 and ROUGE-L scores also reflect strong overlap in vocabulary and sentence structure.\nThe Word Error Rate is just 12.43%, which is excellent for real-world speech and unedited audio.\nWhisper handled Indian English fluently, captured the instructional tone well, and only struggled with minor phrasing and punctuation.\nThis confirms Whisperâ€™s suitability for academic transcription, content indexing, and lecture summarization tasks."
  },
  {
    "objectID": "NLP Speech.html#transcript-comparison-whisper-vs-original",
    "href": "NLP Speech.html#transcript-comparison-whisper-vs-original",
    "title": "",
    "section": "ğŸ“ Transcript Comparison â€“ Whisper vs Original",
    "text": "ğŸ“ Transcript Comparison â€“ Whisper vs Original\nğŸ™ï¸ Sample: NPTEL Lecture â€“ Taylor Series Intro\nğŸ“Œ Original:\n&gt; Welcome back to the lectures on Engineering Mathematics-I.\n&gt; Today, we will learn Taylorâ€™s Polynomial and Taylor Series.\nğŸŒ€ Whisper Output:\n&gt; Hi, welcome back to the lectures on Engineering Mathematics I and todayâ€™s we will learn Taylorâ€™s polynomial and Taylor series.\nğŸ”¹ âœ… Core message preserved\nğŸ”¹ Minor changes:\n- Added â€œHiâ€\n- â€œtodayâ€™s weâ€ instead of â€œtoday weâ€\n- Punctuation differences only"
  },
  {
    "objectID": "NLP Speech.html#section-1",
    "href": "NLP Speech.html#section-1",
    "title": "",
    "section": "",
    "text": "ğŸ™ï¸ Sample: Exponential Function Example\nğŸ“Œ Original:\n&gt; The polynomial of degree 0 will simply be 1.\n&gt; If we plot this, itâ€™s the green line through the point (0, 1).\nğŸŒ€ Whisper Output:\n&gt; So, the polynomial of degree 0 will be simply 1 and if we plot this. So, this is the green plot here of the exponential function and this polynomial of degree 0 is just a constant line. So, the straight line going through this 0 1 point.\nğŸ”¹ âœ… Richer phrasing from audio\nğŸ”¹ Whisper added spontaneous repetitions and fillers (â€œsoâ€, â€œhereâ€)\nğŸ”¹ All technical meaning retained\n\nThese examples show that Whisper Medium provides a high-fidelity transcription of spoken technical content.\nIn the first clip, it added a conversational â€œHiâ€ and reordered â€œtodayâ€™s weâ€ â€” a typical STT quirk, but the meaning is perfectly preserved.\nIn the second clip, Whisper captured additional spoken context and filler words like â€œso,â€ which were omitted in the polished text. Still, the math-specific phrasing and structure â€” like â€œdegree 0 polynomialâ€ and â€œpoint (0, 1)â€ â€” were accurate.\nOverall, Whisper handled clear, structured academic English very well, even in continuous lecture format."
  },
  {
    "objectID": "NLP Speech.html#whisper-accuracy-1",
    "href": "NLP Speech.html#whisper-accuracy-1",
    "title": "",
    "section": "ğŸ“Š Whisper Accuracy",
    "text": "ğŸ“Š Whisper Accuracy\nğŸ”¹ Quantitative Evaluation\n\nSemantic Similarity (cosine): 0.8882\n\nROUGE-1 F1 Score: 0.9116\n\nROUGE-L F1 Score: 0.8913\n\nWord Error Rate (WER): 26.33%\n\nğŸ” Interpretation\n\nâœ… Meaning mostly preserved, despite phrasing variation\n\nâš ï¸ Slight drop in surface-level overlap (function words, rewording)\n\nâš ï¸ Higher WER due to longer output and added filler words\n\n\nIn this earlier test, Whisper Medium still performed well, but not as strongly as in the second test.\nThe semantic similarity score of 0.8882 indicates that the transcript largely matched the original in meaning â€” even though phrasing and filler words may differ.\nROUGE scores are also strong, though slightly lower, likely because Whisper introduced additional spoken-style elements like filler words and partial repetitions that were not in the polished reference transcript.\nThe Word Error Rate of 26.33% is higher, but most errors were minor â€” missing function words, punctuation, or changes in word order â€” rather than major content issues.\nThis reflects Whisperâ€™s tendency to follow real speech patterns closely, which may differ from cleaned-up transcripts."
  },
  {
    "objectID": "NLP Speech.html#conclusion-summary",
    "href": "NLP Speech.html#conclusion-summary",
    "title": "",
    "section": "Conclusion & Summary",
    "text": "Conclusion & Summary\n\nSTT is a mature, versatile technology\nOpen-source tools (Whisper, Wav2Vec) offer high quality with no cost\nCloud APIs provide convenience but incur recurring costs\nModel choice depends on:\nâœ… Accuracy\nâœ… Cost\nâœ… Deployment constraints\nâœ… Privacy needs\n\nâœ”ï¸ Recommended:\nUse Whisper for secure, offline transcription\nUse cloud APIs only where real-time & scalability are critical\n\nTo wrap up:\nSpeech-to-text is now a reliable and accessible technology, with options for both local and cloud-based use cases. Open-source models like Whisper and Wav2Vec 2.0 offer high performance without licensing fees, making them excellent choices for internal tools or data-sensitive applications.\nCloud APIs are valuable when speed, scale, or language diversity is essential â€” but they should be weighed against cost and privacy considerations.\nUltimately, the choice depends on our projectâ€™s priorities â€” whether thatâ€™s minimizing cost, protecting data, supporting real-time use, or ensuring high accuracy.\nThank you."
  },
  {
    "objectID": "NLP Speech.html#thank-you",
    "href": "NLP Speech.html#thank-you",
    "title": "",
    "section": "Thank You!",
    "text": "Thank You!\nQuestions? Suggestions?\nğŸ”— View this presentation on GitHub:\nPresented by Omkar Ninav â€” June 2025\n\nThank you for your attention! Iâ€™m happy to take questions, discuss possible use cases for STT in our workflows, or explore ways we can experiment with tools like Whisper in-house.\nThe full slide deck and source code are available on GitHub for review, reuse, or contributions.\n\n\n\n\nInternal Use Only | Not For Redistribution"
  }
]